{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242b54c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tensorflow as tf\\nfrom tensorflow.nn import relu, dropout\\n\\nclass GCN(tf.keras.Model):\\n  def __init__(self, in_features, hidden_features, dropout_rate):\\n    super(GCN, self).__init__()\\n\\n    self.fc1 = tf.keras.layers.Dense(hidden_features, activation=relu)\\n    self.fc2 = tf.keras.layers.Dense(1, activation=\"sigmoid\")  # Sigmoid for link probability\\n    self.dropout = dropout(dropout_rate)\\n\\n  def call(self, inputs):\\n    x1, x2, adj = inputs  # source features, destination features, adjacency matrix\\n\\n    # Combine source and destination features\\n    node_features = tf.concat([x1, x2], axis=1)\\n\\n    # ... (message passing with adj as before)\\n\\n    # Output layer for link prediction\\n    output = self.dropout(self.fc2(hidden))\\n    return output\\n\\n# Example usage\\nmodel = GCN(16, 8, 0.5)  # in_features, hidden_features, dropout\\n\\n# ... (Load features, adjacency matrix, prepare positive/negative samples)\\n\\n# Train/Validation split (using your preferred splitting method)\\ntrain_features_source, train_features_dest, train_adj, train_labels = ...\\nval_features_source, val_features_dest, val_adj, val_labels = ...\\ntest_features_source, test_features_dest, test_adj = ...\\n\\n# Compile model for training\\noptimizer = tf.keras.optimizers.Adam()\\nloss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\\n\\n# Training loop (using train/val data)\\n# ...\\n\\n# Testing loop (using test data)\\nmodel.evaluate((test_features_source, test_features_dest, test_adj), test_labels)\\n\\n# Prediction on new unseen edges\\nnew_source_features, new_dest_features = ...  # Features for new potential links\\npredictions = model((new_source_features, new_dest_features, test_adj))\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import tensorflow as tf\n",
    "from tensorflow.nn import relu, dropout\n",
    "\n",
    "class GCN(tf.keras.Model):\n",
    "  def __init__(self, in_features, hidden_features, dropout_rate):\n",
    "    super(GCN, self).__init__()\n",
    "\n",
    "    self.fc1 = tf.keras.layers.Dense(hidden_features, activation=relu)\n",
    "    self.fc2 = tf.keras.layers.Dense(1, activation=\"sigmoid\")  # Sigmoid for link probability\n",
    "    self.dropout = dropout(dropout_rate)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x1, x2, adj = inputs  # source features, destination features, adjacency matrix\n",
    "\n",
    "    # Combine source and destination features\n",
    "    node_features = tf.concat([x1, x2], axis=1)\n",
    "\n",
    "    # ... (message passing with adj as before)\n",
    "\n",
    "    # Output layer for link prediction\n",
    "    output = self.dropout(self.fc2(hidden))\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "model = GCN(16, 8, 0.5)  # in_features, hidden_features, dropout\n",
    "\n",
    "# ... (Load features, adjacency matrix, prepare positive/negative samples)\n",
    "\n",
    "# Train/Validation split (using your preferred splitting method)\n",
    "train_features_source, train_features_dest, train_adj, train_labels = ...\n",
    "val_features_source, val_features_dest, val_adj, val_labels = ...\n",
    "test_features_source, test_features_dest, test_adj = ...\n",
    "\n",
    "# Compile model for training\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Training loop (using train/val data)\n",
    "# ...\n",
    "\n",
    "# Testing loop (using test data)\n",
    "model.evaluate((test_features_source, test_features_dest, test_adj), test_labels)\n",
    "\n",
    "# Prediction on new unseen edges\n",
    "new_source_features, new_dest_features = ...  # Features for new potential links\n",
    "predictions = model((new_source_features, new_dest_features, test_adj))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d6df0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport tensorflow as tf\\nfrom tensorflow.nn import relu, dropout\\n\\n# Function to read CSV and create data structures\\ndef process_data(csv_file):\\n  df = pd.read_csv(csv_file)\\n  embedding_dict = {}\\n  adj_matrix = tf.zeros((max_node_id + 1, max_node_id + 1))  # Assuming node IDs start from 0\\n\\n  for index, row in df.iterrows():\\n    node_id = row[\"node_id\"]  # Replace with your actual column name for node ID\\n    embedding = row[embedding_feature_columns]  # Replace with column names for embedding\\n    embedding_dict[node_id] = embedding\\n\\n    # Process connections (assuming separate columns for source and destination)\\n    source = row[\"source_node_id\"]  # Replace with your actual column name\\n    dest = row[\"destination_node_id\"]  # Replace with your actual column name\\n    adj_matrix[source, dest] = 1  # Update adjacency matrix for connections\\n\\n  return embedding_dict, adj_matrix\\n\\n# ... (Load your CSV file)\\n\\nembedding_dict, adj_matrix = process_data(csv_file)\\n\\n# Function to prepare training and testing data (replace with your logic for negative sampling)\\ndef prepare_data(embedding_dict, adj_matrix, test_ratio=0.2):\\n  # Extract positive samples from connection information\\n  positive_samples = []\\n  for source, row in embedding_dict.items():\\n    for dest in row.keys():  # Assuming connections are stored in a dictionary within embeddings\\n      if source != dest:\\n        positive_samples.append((source, dest))\\n\\n  # Generate negative samples (replace with your preferred negative sampling method)\\n  negative_samples = [(source, random.choice(list(embedding_dict.keys()))) for source, _ in positive_samples]\\n\\n  # Split data into training and testing sets\\n  train_size = int(len(positive_samples) * (1 - test_ratio))\\n  train_data = positive_samples[:train_size] + negative_samples[:train_size]\\n  test_data = positive_samples[train_size:] + negative_samples[train_size:]\\n\\n  return train_data, test_data\\n\\n# ... (Prepare training and testing data using your logic for negative sampling)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.nn import relu, dropout\n",
    "\n",
    "# Function to read CSV and create data structures\n",
    "def process_data(csv_file):\n",
    "  df = pd.read_csv(csv_file)\n",
    "  embedding_dict = {}\n",
    "  adj_matrix = tf.zeros((max_node_id + 1, max_node_id + 1))  # Assuming node IDs start from 0\n",
    "\n",
    "  for index, row in df.iterrows():\n",
    "    node_id = row[\"node_id\"]  # Replace with your actual column name for node ID\n",
    "    embedding = row[embedding_feature_columns]  # Replace with column names for embedding\n",
    "    embedding_dict[node_id] = embedding\n",
    "\n",
    "    # Process connections (assuming separate columns for source and destination)\n",
    "    source = row[\"source_node_id\"]  # Replace with your actual column name\n",
    "    dest = row[\"destination_node_id\"]  # Replace with your actual column name\n",
    "    adj_matrix[source, dest] = 1  # Update adjacency matrix for connections\n",
    "\n",
    "  return embedding_dict, adj_matrix\n",
    "\n",
    "# ... (Load your CSV file)\n",
    "\n",
    "embedding_dict, adj_matrix = process_data(csv_file)\n",
    "\n",
    "# Function to prepare training and testing data (replace with your logic for negative sampling)\n",
    "def prepare_data(embedding_dict, adj_matrix, test_ratio=0.2):\n",
    "  # Extract positive samples from connection information\n",
    "  positive_samples = []\n",
    "  for source, row in embedding_dict.items():\n",
    "    for dest in row.keys():  # Assuming connections are stored in a dictionary within embeddings\n",
    "      if source != dest:\n",
    "        positive_samples.append((source, dest))\n",
    "\n",
    "  # Generate negative samples (replace with your preferred negative sampling method)\n",
    "  negative_samples = [(source, random.choice(list(embedding_dict.keys()))) for source, _ in positive_samples]\n",
    "\n",
    "  # Split data into training and testing sets\n",
    "  train_size = int(len(positive_samples) * (1 - test_ratio))\n",
    "  train_data = positive_samples[:train_size] + negative_samples[:train_size]\n",
    "  test_data = positive_samples[train_size:] + negative_samples[train_size:]\n",
    "\n",
    "  return train_data, test_data\n",
    "\n",
    "# ... (Prepare training and testing data using your logic for negative sampling)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21b0a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"neo4jkngbq\"\n",
    "\n",
    "from py2neo import Graph\n",
    "\n",
    "# Connect to Neo4j\n",
    "graph = Graph(url, auth=(user, password))\n",
    "\n",
    "# Query to get node embeddings and relationships\n",
    "query = \"\"\"\n",
    "MATCH (n:SubNode{filename:\"software\"})-[r]->(m:Interface)\n",
    "RETURN id(n) AS source_id, id(m) AS target_id, n.embeddings AS src_embedding, m.embeddings AS tgt_embedding\n",
    "\"\"\"\n",
    "\n",
    "data = graph.run(query).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952a663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66d46e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_embedding_str_to_list(embedding_str):\n",
    "    emb_split = embedding_str.split(',')\n",
    "    emb_list = [float(x) for x in emb_split]\n",
    "    return emb_list\n",
    "\n",
    "node_embeddings = {}\n",
    "for d in data:\n",
    "    src_id = d['source_id']\n",
    "    tgt_id = d['target_id']\n",
    "    if src_id not in node_embeddings:\n",
    "        node_embeddings[src_id] = convert_embedding_str_to_list(d['src_embedding'])\n",
    "    if tgt_id not in node_embeddings:\n",
    "        node_embeddings[tgt_id] = convert_embedding_str_to_list(d['tgt_embedding'])\n",
    "        \n",
    "node_id_mapping = {node_id: idx for idx, node_id in enumerate(node_embeddings.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db22d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "    [node_id_mapping[d['source_id']], node_id_mapping[d['target_id']]]\n",
    "    for d in data\n",
    "], dtype=torch.long).t().contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d81084a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = torch.tensor([node_embeddings[node_id] for node_id in sorted(node_embeddings.keys())], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92b56356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Creating the PyG data object\n",
    "graph_data = Data(x=node_features, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "896a5e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[424, 1536], edge_index=[2, 1182])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "574d0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "# Split edges and prepare data loaders\n",
    "transform_split = RandomLinkSplit(is_undirected=True, add_negative_train_samples=True, num_val=0.1, num_test=0.1)\n",
    "train_data, val_data, test_data = transform_split(graph_data)\n",
    "\n",
    "# Assuming you need to manually handle subgraph extraction if required:\n",
    "# You would typically prepare subgraphs manually or use a custom dataset class here\n",
    "# For now, let's proceed without the subgraph extraction to focus on other parts of the setup\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28cb84db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[424, 1536], edge_index=[2, 168], edge_label=[18], edge_label_index=[2, 18])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd1036c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[424, 1536], edge_index=[2, 150], edge_label=[18], edge_label_index=[2, 18])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "03fb07ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[424, 1536], edge_index=[2, 150], edge_label=[150], edge_label_index=[2, 150])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c15ebc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data.x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "88b640c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[424, 1536], edge_index=[2, 150], edge_label=[150], edge_label_index=[2, 150])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6f1e996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'follow_batch': None,\n",
       " 'exclude_keys': None,\n",
       " 'dataset': Data(x=[424, 1536], edge_index=[2, 150], edge_label=[150], edge_label_index=[2, 150]),\n",
       " 'num_workers': 0,\n",
       " 'prefetch_factor': None,\n",
       " 'pin_memory': False,\n",
       " 'pin_memory_device': '',\n",
       " 'timeout': 0,\n",
       " 'worker_init_fn': None,\n",
       " '_DataLoader__multiprocessing_context': None,\n",
       " '_dataset_kind': 0,\n",
       " 'batch_size': 32,\n",
       " 'drop_last': False,\n",
       " 'sampler': <torch.utils.data.sampler.RandomSampler at 0x338de0950>,\n",
       " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x338de6fd0>,\n",
       " 'generator': None,\n",
       " 'collate_fn': <torch_geometric.loader.dataloader.Collater at 0x338cdd410>,\n",
       " 'persistent_workers': False,\n",
       " '_DataLoader__initialized': True,\n",
       " '_IterableDataset_len_called': None,\n",
       " '_iterator': None}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8bf03135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[424, 1536], edge_index=[2, 150], edge_label=[150], edge_label_index=[2, 150])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d1f106aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class SEALModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(SEALModel, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels//2)\n",
    "        self.conv3 = GCNConv(hidden_channels//2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x.view(-1)\n",
    "\n",
    "model = SEALModel(len(test_data.x[1]), 256)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd851b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train ():\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "34f4ead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', tensor([[-0.0223, -0.0170,  0.0130,  ..., -0.0061,  0.0018, -0.0123],\n",
      "        [-0.0250, -0.0242,  0.0024,  ..., -0.0373, -0.0035,  0.0080],\n",
      "        [-0.0214, -0.0113,  0.0196,  ..., -0.0205, -0.0139,  0.0093],\n",
      "        ...,\n",
      "        [-0.0230, -0.0203,  0.0094,  ..., -0.0122, -0.0089, -0.0315],\n",
      "        [ 0.0021, -0.0144,  0.0180,  ..., -0.0115, -0.0083, -0.0157],\n",
      "        [ 0.0118, -0.0150, -0.0151,  ...,  0.0089, -0.0108, -0.0319]]))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[79], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m      8\u001b[0m label \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, label)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in test_data:\n",
    "        print(data)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        label = data.y.float()\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch+1}: Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02f655dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric.deprecation.DataLoader at 0x16d635590>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149a815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
